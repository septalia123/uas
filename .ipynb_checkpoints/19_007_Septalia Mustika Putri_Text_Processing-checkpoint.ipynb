{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c020d3",
   "metadata": {
    "id": "96c020d3"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6669bb2",
   "metadata": {
    "id": "b6669bb2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re #regrex libray\n",
    "import nltk\n",
    "import swifter\n",
    "import Sastrawi\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205dda63",
   "metadata": {
    "id": "205dda63"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0ef2b6",
   "metadata": {
    "id": "2c0ef2b6",
    "outputId": "ba5ade28-be64-4194-f342-900433f45d7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>waktu</th>\n",
       "      <th>category</th>\n",
       "      <th>isi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wapres Ungkap Data Amerika Soal Ketahanan Pang...</td>\n",
       "      <td>Selasa, 22 Maret 2022 - 17:14 WIB</td>\n",
       "      <td>Sektor Riil</td>\n",
       "      <td>Wakil Presiden Maruf Amin mengungkapkan, berda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lebih Murah, YLKI Khawatir Konsumen Bakal Migr...</td>\n",
       "      <td>Selasa, 22 Maret 2022 - 16:48 WIB</td>\n",
       "      <td>Sektor Riil</td>\n",
       "      <td>Ketua Yayasan Lembaga Konsumen Indonesia (YLKI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kemenkeu Siapkan 186 Pasal yang Mengatur Penda...</td>\n",
       "      <td>Selasa, 22 Maret 2022 - 16:24 WIB</td>\n",
       "      <td>Makro</td>\n",
       "      <td>Kementerian Keuangan tengah menyiapkan 186 pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Menghijau, IHSG Hari Ini Ditutup Tembus Level ...</td>\n",
       "      <td>Selasa, 22 Maret 2022 - 15:37 WIB</td>\n",
       "      <td>Kurs &amp; Saham</td>\n",
       "      <td>Indeks Harga Saham Gabungan (IHSG) hari ini me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tumbuh Berlipat dari PDB, Ekonomi Digital Jadi...</td>\n",
       "      <td>Selasa, 22 Maret 2022 - 15:29 WIB</td>\n",
       "      <td>Makro</td>\n",
       "      <td>Laju pertumbuhan ekonomi digital Indonesia dip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               judul  \\\n",
       "0  Wapres Ungkap Data Amerika Soal Ketahanan Pang...   \n",
       "1  Lebih Murah, YLKI Khawatir Konsumen Bakal Migr...   \n",
       "2  Kemenkeu Siapkan 186 Pasal yang Mengatur Penda...   \n",
       "3  Menghijau, IHSG Hari Ini Ditutup Tembus Level ...   \n",
       "4  Tumbuh Berlipat dari PDB, Ekonomi Digital Jadi...   \n",
       "\n",
       "                               waktu      category  \\\n",
       "0  Selasa, 22 Maret 2022 - 17:14 WIB   Sektor Riil   \n",
       "1  Selasa, 22 Maret 2022 - 16:48 WIB   Sektor Riil   \n",
       "2  Selasa, 22 Maret 2022 - 16:24 WIB         Makro   \n",
       "3  Selasa, 22 Maret 2022 - 15:37 WIB  Kurs & Saham   \n",
       "4  Selasa, 22 Maret 2022 - 15:29 WIB         Makro   \n",
       "\n",
       "                                                 isi  \n",
       "0  Wakil Presiden Maruf Amin mengungkapkan, berda...  \n",
       "1  Ketua Yayasan Lembaga Konsumen Indonesia (YLKI...  \n",
       "2  Kementerian Keuangan tengah menyiapkan 186 pas...  \n",
       "3  Indeks Harga Saham Gabungan (IHSG) hari ini me...  \n",
       "4  Laju pertumbuhan ekonomi digital Indonesia dip...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_berita = pd.read_csv('wrapping-text.csv')\n",
    "data_berita.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7a628",
   "metadata": {
    "id": "dcd7a628"
   },
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449b1906",
   "metadata": {
    "id": "449b1906",
    "outputId": "bf89b641-c330-4348-8610-8fc7466cfa3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0     wakil presiden maruf amin mengungkapkan, berda...\n",
      "1     ketua yayasan lembaga konsumen indonesia (ylki...\n",
      "2     kementerian keuangan tengah menyiapkan 186 pas...\n",
      "3     indeks harga saham gabungan (ihsg) hari ini me...\n",
      "4     laju pertumbuhan ekonomi digital indonesia dip...\n",
      "5     menyusul dicabutnya aturan harga eceran tertin...\n",
      "6     menteri keuangan (menkeu) sri mulyani menyatak...\n",
      "7     menteri keuangan (menkeu) sri mulyani merespon...\n",
      "8     harga minyak mentah atau crude oil mengalami k...\n",
      "9     menko airlangga mengakui pelaku umkm menjadi c...\n",
      "10    asosiasi pengusaha ritel indonesia (aprindo) m...\n",
      "11    indeks harga saham gabungan (ihsg) hari ini me...\n",
      "12    kementerian perhubungan menyatakan belum ada k...\n",
      "13    aturan untuk memulai pembangunan ibu kota nega...\n",
      "14    otoritas jasa keuangan mendorong perusahaan bu...\n",
      "15    pencabutan harga eceran tertinggi (het) minyak...\n",
      "16    berdasarkan catatan otoritas jasa keuangan (oj...\n",
      "17    wakil ketua dpr ri, rachmat gobel menyatakan, ...\n",
      "18    pt goto gojek tokopedia tbk atau goto memperpa...\n",
      "19    ihsg hari ini dibuka menguat di 6.974,97 pada ...\n",
      "20    harga emas batangan pt aneka tambang (antam) t...\n",
      "21    81 perusahaan industri minyak goreng wajib unt...\n",
      "22    fenomena ghozali everyday yang sukses menjual ...\n",
      "23    masyarakat tidak perlu khawatir dan tidak perl...\n",
      "24    menteri koordinator maritim dan investasi, luh...\n",
      "25    indeks harga saham gabungan atau ihsg hari ini...\n",
      "26    para pemimpin uni eropa (ue) menemui jalan bun...\n",
      "27    investor asing terpantau melakukan aksi penjua...\n",
      "28    selain kenaikan ppn, pemerintah juga akan mene...\n",
      "29    dow jones industrial average turun 85,08 poin ...\n",
      "Name: isi, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "data_berita['isi'] = data_berita['isi'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(data_berita['isi'].head(30))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728553b",
   "metadata": {
    "id": "b728553b"
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5894ad6",
   "metadata": {
    "id": "b5894ad6",
    "outputId": "3e1a4a0b-5f57-48a6-df8c-3d1030da0913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0     [wakil, presiden, maruf, amin, mengungkapkan, ...\n",
      "1     [ketua, yayasan, lembaga, konsumen, indonesia,...\n",
      "2     [kementerian, keuangan, tengah, menyiapkan, pa...\n",
      "3     [indeks, harga, saham, gabungan, ihsg, hari, i...\n",
      "4     [laju, pertumbuhan, ekonomi, digital, indonesi...\n",
      "5     [menyusul, dicabutnya, aturan, harga, eceran, ...\n",
      "6     [menteri, keuangan, menkeu, sri, mulyani, meny...\n",
      "7     [menteri, keuangan, menkeu, sri, mulyani, mere...\n",
      "8     [harga, minyak, mentah, atau, crude, oil, meng...\n",
      "9     [menko, airlangga, mengakui, pelaku, umkm, men...\n",
      "10    [asosiasi, pengusaha, ritel, indonesia, aprind...\n",
      "11    [indeks, harga, saham, gabungan, ihsg, hari, i...\n",
      "12    [kementerian, perhubungan, menyatakan, belum, ...\n",
      "13    [aturan, untuk, memulai, pembangunan, ibu, kot...\n",
      "14    [otoritas, jasa, keuangan, mendorong, perusaha...\n",
      "15    [pencabutan, harga, eceran, tertinggi, het, mi...\n",
      "16    [berdasarkan, catatan, otoritas, jasa, keuanga...\n",
      "17    [wakil, ketua, dpr, ri, rachmat, gobel, menyat...\n",
      "18    [pt, goto, gojek, tokopedia, tbk, atau, goto, ...\n",
      "19    [ihsg, hari, ini, dibuka, menguat, di, pada, p...\n",
      "20    [harga, emas, batangan, pt, aneka, tambang, an...\n",
      "21    [perusahaan, industri, minyak, goreng, wajib, ...\n",
      "22    [fenomena, ghozali, everyday, yang, sukses, me...\n",
      "23    [masyarakat, tidak, perlu, khawatir, dan, tida...\n",
      "24    [menteri, koordinator, maritim, dan, investasi...\n",
      "25    [indeks, harga, saham, gabungan, atau, ihsg, h...\n",
      "26    [para, pemimpin, uni, eropa, ue, menemui, jala...\n",
      "27    [investor, asing, terpantau, melakukan, aksi, ...\n",
      "28    [selain, kenaikan, ppn, pemerintah, juga, akan...\n",
      "29    [dow, jones, industrial, average, turun, poin,...\n",
      "Name: deskripsi_tokens, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "data_berita['isi'] = data_berita['isi'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data_berita['deskripsi_tokens'] = data_berita['isi'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(data_berita['deskripsi_tokens'].head(30))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5036a5b",
   "metadata": {
    "id": "b5036a5b"
   },
   "source": [
    "# Menghitung Frekuensi Distribusi Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2318a10",
   "metadata": {
    "id": "d2318a10",
    "outputId": "60df5f12-4c89-41af-8e5f-87a68b53db20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0     [(wakil, 1), (presiden, 1), (maruf, 1), (amin,...\n",
      "1     [(ketua, 1), (yayasan, 1), (lembaga, 1), (kons...\n",
      "2     [(kementerian, 1), (keuangan, 1), (tengah, 1),...\n",
      "3     [(indeks, 2), (harga, 1), (saham, 1), (gabunga...\n",
      "4     [(pertumbuhan, 2), (laju, 1), (ekonomi, 1), (d...\n",
      "5     [(harga, 2), (menyusul, 1), (dicabutnya, 1), (...\n",
      "6     [(ppn, 2), (menteri, 1), (keuangan, 1), (menke...\n",
      "7     [(harga, 2), (minyak, 2), (menteri, 1), (keuan...\n",
      "8     [(pada, 2), (harga, 1), (minyak, 1), (mentah, ...\n",
      "9     [(nasional, 2), (menko, 1), (airlangga, 1), (m...\n",
      "10    [(ritel, 2), (asosiasi, 1), (pengusaha, 1), (i...\n",
      "11    [(indeks, 1), (harga, 1), (saham, 1), (gabunga...\n",
      "12    [(terkait, 2), (mudik, 2), (kementerian, 1), (...\n",
      "13    [(untuk, 2), (aturan, 1), (memulai, 1), (pemba...\n",
      "14    [(perusahaan, 3), (bumn, 2), (otoritas, 1), (j...\n",
      "15    [(minyak, 2), (goreng, 2), (pencabutan, 1), (h...\n",
      "16    [(perusahaan, 2), (berdasarkan, 1), (catatan, ...\n",
      "17    [(wakil, 1), (ketua, 1), (dpr, 1), (ri, 1), (r...\n",
      "18    [(goto, 2), (pt, 1), (gojek, 1), (tokopedia, 1...\n",
      "19    [(ihsg, 2), (hari, 2), (ini, 2), (dibuka, 2), ...\n",
      "20    [(pada, 2), (harga, 1), (emas, 1), (batangan, ...\n",
      "21    [(minyak, 3), (goreng, 3), (curah, 2), (masyar...\n",
      "22    [(fenomena, 1), (ghozali, 1), (everyday, 1), (...\n",
      "23    [(tidak, 2), (perlu, 2), (dan, 2), (tetap, 2),...\n",
      "24    [(dan, 2), (menteri, 1), (koordinator, 1), (ma...\n",
      "25    [(indeks, 1), (harga, 1), (saham, 1), (gabunga...\n",
      "26    [(para, 1), (pemimpin, 1), (uni, 1), (eropa, 1...\n",
      "27    [(investor, 1), (asing, 1), (terpantau, 1), (m...\n",
      "28    [(yang, 2), (selain, 1), (kenaikan, 1), (ppn, ...\n",
      "29    [(dow, 1), (jones, 1), (industrial, 1), (avera...\n",
      "Name: deskripsi_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "data_berita['deskripsi_tokens_fdist'] = data_berita['deskripsi_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(data_berita['deskripsi_tokens_fdist'].head(30).apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77b94c",
   "metadata": {
    "id": "fa77b94c"
   },
   "source": [
    "# Filtering (Stopword Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5354eade",
   "metadata": {
    "id": "5354eade",
    "outputId": "0020fa70-7fe3-49c0-f2f1-afbbf0a07719"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scrapy_berita.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9800/2497553374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# ----------------------- add stopword from txt file ------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# read txt stopword using pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtxt_stopword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scrapy_berita.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"stopwords\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# convert stopword string to list & append additional stopword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scrapy_berita.csv'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"scrapy_berita.csv\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "data_berita['deskripsi_tokens_WSW'] = data_berita['deskripsi_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(data_berita['deskripsi_tokens_WSW'].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a9cdb",
   "metadata": {
    "id": "913a9cdb"
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee05c81",
   "metadata": {
    "id": "6ee05c81",
    "outputId": "d2a665d6-3327-418a-8802-6303cc97140c"
   },
   "outputs": [],
   "source": [
    "normalizad_word = pd.read_csv(\"scrapy_berita.csv\")\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "data_berita['deskripsi_normalized'] = data_berita['deskripsi_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "data_berita['deskripsi_normalized'].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb23492",
   "metadata": {
    "id": "0eb23492"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad76fc38",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b4614e9de1544f4c8e3ca4b0c9a1fa0c"
     ]
    },
    "id": "ad76fc38",
    "outputId": "db9246a2-bf71-4e82-ee52-8fa7ff66a558"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'deskripsi_normalized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'deskripsi_normalized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9800/3672860161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mterm_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_berita\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deskripsi_normalized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mterm_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'deskripsi_normalized'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data_berita['deskripsi_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data_berita['deskripsi_tokens_stemmed'] = data_berita['deskripsi_normalized'].swifter.apply(get_stemmed_term)\n",
    "print(data_berita['deskripsi_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9dac3",
   "metadata": {
    "id": "d0e9dac3"
   },
   "source": [
    "# Simpan Data ke CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80f2d0",
   "metadata": {
    "id": "4e80f2d0"
   },
   "outputs": [],
   "source": [
    "data_berita.to_csv(\"Text_PreprocessingBaru.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf12b8",
   "metadata": {
    "id": "42bf12b8"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c1802b8",
   "metadata": {
    "id": "7c1802b8",
    "outputId": "62372147-49f3-48d7-deaf-c33404be3003"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Text_PreprocessingBaru.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9800/2013607673.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_berita\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Text_PreprocessingBaru.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"deskripsi_tokens_stemmed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata_berita\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"deskripsi\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Text_PreprocessingBaru.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data_berita = pd.read_csv(\"Text_PreprocessingBaru.csv\", usecols=[\"deskripsi_tokens_stemmed\"])\n",
    "data_berita.columns = [\"deskripsi\"]\n",
    "\n",
    "data_berita.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73971e0e",
   "metadata": {
    "id": "73971e0e"
   },
   "source": [
    "# Term Frekuensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ccdbe00",
   "metadata": {
    "id": "5ccdbe00",
    "outputId": "8c89369f-402c-4291-a11a-12d4c7e5fbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'fraksi': 66, 'pks': 185, 'dpr': 55, 'ada': 1, 'konsolidasi': 123, 'nasional': 159, 'pimpin': 183, 'dprd': 56, 'indonesia': 83, 'mantap': 142, 'kerja': 114, 'milu': 153, 'isu': 87, 'ombak': 169, 'reshuffle': 215, 'kabinet': 104, 'cuat': 42, 'muka': 157, 'publik': 201, 'kabar': 103, 'presiden': 199, 'jokowi': 101, 'kocok': 118, 'ulang': 265, 'rabu': 207, 'komandan': 119, 'korps': 124, 'marinir': 146, 'mayjen': 149, 'tni': 258, 'mar': 144, 'widodo': 269, 'dwi': 60, 'purwanto': 205, 'lantik': 131, 'februari': 65, 'hadap': 71, 'panglima': 170, 'jenderal': 99, 'andika': 15, 'perkasa': 179, 'mabes': 137, 'ketua': 115, 'komisi': 120, 'pilih': 182, 'kpu': 127, 'ilham': 82, 'saputra': 228, 'tunda': 261, 'lanjut': 130, 'tahap': 250, 'sesuai': 236, 'jadwal': 91, 'era': 63, 'disrupsi': 51, 'inovasi': 84, 'manfaat': 140, 'teknologi': 253, 'bisnis': 31, 'jasa': 96, 'mampu': 139, 'adaptasi': 2, 'kpk': 126, 'selidik': 233, 'ektp': 62, 'duga': 57, 'rugi': 220, 'uang': 262, 'negara': 161, 'triliun': 260, 'rupiah': 221, 'perum': 180, 'cetak': 38, 'republik': 214, 'pnri': 187, 'polri': 191, 'ralat': 210, 'nyata': 167, 'kait': 106, 'gilang': 69, 'widya': 270, 'pramana': 197, 'juragan': 102, 'lapor': 132, 'bareskrim': 25, 'tipu': 257, 'jahat': 93, 'rahasia': 208, 'dagang': 44, 'akademisi': 9, 'universitas': 267, 'paramadina': 172, 'yudi': 271, 'latif': 133, 'sistem': 241, 'demokrasi': 48, 'sungguh': 246, 'prosedural': 200, 'rusak': 222, 'gelar': 67, 'simulasi': 240, 'sederhana': 230, 'surat': 248, 'suara': 244, 'halaman': 74, 'kantor': 107, 'ri': 216, 'jakarta': 94, 'selasa': 232, 'puncak': 204, 'haul': 77, 'kh': 116, 'hasyim': 76, 'muzadi': 158, 'masjid': 148, 'pesantren': 181, 'alhikam': 11, 'depok': 49, 'jawa': 97, 'barat': 24, 'minggu': 154, 'karo': 108, 'penmas': 175, 'divisi': 52, 'humas': 80, 'brigjen': 32, 'pol': 188, 'ahmad': 6, 'ramadhan': 211, 'kenal': 112, 'sidik': 238, 'berantas': 27, 'korupsi': 125, 'agenda': 5, 'periksa': 176, 'mantan': 141, 'ppp': 195, 'romahurmuziy': 219, 'kepala': 113, 'pusat': 206, 'analisis': 13, 'transaksi': 259, 'ppatk': 194, 'ivan': 89, 'yustiavandana': 272, 'antisipasi': 17, 'money': 155, 'politics': 189, 'politik': 190, 'jelang': 98, 'staf': 243, 'angkat': 16, 'udara': 263, 'ksau': 128, 'marsekal': 147, 'fadjar': 64, 'prasetyo': 198, 'kunjung': 129, 'nostalgia': 165, 'australian': 21, 'defence': 47, 'college': 39, 'adc': 3, 'alumnus': 12, 'panjang': 171, 'tahan': 249, 'hakim': 73, 'nonaktif': 164, 'adil': 4, 'negeri': 162, 'pn': 186, 'surabaya': 247, 'itong': 88, 'isnaeni': 86, 'hidayat': 78, 'iih': 81, 'perintah': 177, 'menteri': 150, 'hukum': 79, 'hak': 72, 'asasi': 20, 'manusia': 143, 'kemenkumham': 111, 'komitmen': 121, 'dukung': 58, 'pulih': 202, 'ekonomi': 61, 'upayaupaya': 268, 'dorong': 54, 'tingkat': 256, 'potensi': 192, 'kaya': 109, 'intelektual': 85, 'ki': 117, 'celaka': 36, 'lintas': 136, 'salah': 224, 'sebab': 229, 'tinggal': 255, 'dunia': 59, 'besar': 28, 'dasar': 45, 'data': 46, 'tahun': 251, 'puluh': 203, 'ribu': 217, 'nyawa': 168, 'layang': 134, 'siasia': 237, 'jalan': 95, 'undur': 266, 'rais': 209, 'aam': 0, 'pbnu': 174, 'miftachul': 151, 'akhyar': 10, 'jabat': 90, 'majelis': 138, 'ulama': 264, 'mui': 156, 'nilai': 163, 'representasi': 213, 'nu': 166, 'temu': 254, 'bilateral': 29, 'hari': 75, 'air': 7, 'and': 14, 'space': 242, 'power': 193, 'conference': 40, 'national': 160, 'convention': 41, 'centre': 37, 'canbera': 34, 'senin': 234, 'sila': 239, 'badan': 23, 'awas': 22, 'bawaslu': 26, 'milik': 152, 'rencana': 212, 'gugat': 70, 'pkpu': 184, 'daftar': 43, 'partai': 173, 'calon': 33, 'serta': 235, 'peristiwa': 178, 'sejarah': 231, 'tanggal': 252, 'maret': 145, 'diri': 50, 'liga': 135, 'arab': 18, 'artis': 19, 'rizky': 218, 'billar': 30, 'jerat': 100, 'doni': 53, 'salmanan': 225, 'sandiaga': 227, 'kader': 105, 'gerindra': 68, 'sama': 226, 'prabowo': 196, 'subianto': 245, 'jago': 92, 'capres': 35, 'ajar': 8, 'konflik': 122, 'rusiaukraina': 223, 'kembang': 110}\n",
      "Encoded Document is:\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a=len(document)\n",
    "document = data_berita['deskripsi']\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(document)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7abf5146",
   "metadata": {
    "id": "7abf5146"
   },
   "outputs": [],
   "source": [
    "a = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac3629",
   "metadata": {
    "id": "2dac3629"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c37e395",
   "metadata": {
    "id": "3c37e395"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "tf = tfidf.fit_transform(vectorizer.fit_transform(document)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e72752",
   "metadata": {
    "id": "23e72752",
    "outputId": "28ee0cfd-0d40-4804-f47f-bfcf73d13034"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>aam</th>\n",
       "      <th>ada</th>\n",
       "      <th>adaptasi</th>\n",
       "      <th>adc</th>\n",
       "      <th>adil</th>\n",
       "      <th>agenda</th>\n",
       "      <th>ahmad</th>\n",
       "      <th>air</th>\n",
       "      <th>ajar</th>\n",
       "      <th>akademisi</th>\n",
       "      <th>...</th>\n",
       "      <th>udara</th>\n",
       "      <th>ulama</th>\n",
       "      <th>ulang</th>\n",
       "      <th>undur</th>\n",
       "      <th>universitas</th>\n",
       "      <th>upayaupaya</th>\n",
       "      <th>widodo</th>\n",
       "      <th>widya</th>\n",
       "      <th>yudi</th>\n",
       "      <th>yustiavandana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218163</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218994</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.234709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261063</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.235943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230736</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.253281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253281</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.253281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aam       ada adaptasi       adc      adil    agenda     ahmad  \\\n",
       "1   0.000000  0.218163  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.28709  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.218994  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.266603   \n",
       "12  0.000000  0.000000  0.00000  0.000000  0.000000  0.321793  0.000000   \n",
       "13  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.00000  0.258241  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.00000  0.000000  0.262225  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.253281  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         air      ajar akademisi  ...     udara     ulama ulang     undur  \\\n",
       "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.25  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "8   0.000000  0.000000  0.261063  ...  0.000000  0.000000  0.00  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "14  0.000000  0.000000  0.000000  ...  0.258241  0.000000  0.00  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "17  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.253281  0.00  0.253281   \n",
       "19  0.255241  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "21  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "22  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "23  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "24  0.000000  0.320837  0.000000  ...  0.000000  0.000000  0.00  0.000000   \n",
       "\n",
       "   universitas upayaupaya   widodo     widya      yudi yustiavandana  \n",
       "1     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "2     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "3     0.000000   0.000000  0.21966  0.000000  0.000000      0.000000  \n",
       "4     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "5     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "6     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "7     0.000000   0.000000  0.00000  0.234709  0.000000      0.000000  \n",
       "8     0.261063   0.000000  0.00000  0.000000  0.261063      0.000000  \n",
       "9     0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "10    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "11    0.000000   0.000000  0.00000  0.235943  0.000000      0.000000  \n",
       "12    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "13    0.000000   0.000000  0.00000  0.000000  0.000000      0.255431  \n",
       "14    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "15    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "16    0.000000   0.230736  0.00000  0.000000  0.000000      0.000000  \n",
       "17    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "18    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "19    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "20    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "21    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "22    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "23    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "24    0.000000   0.000000  0.00000  0.000000  0.000000      0.000000  \n",
       "\n",
       "[24 rows x 273 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb = pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a])\n",
    "dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b60c23",
   "metadata": {
    "id": "36b60c23"
   },
   "source": [
    "# Convert File to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58075c85",
   "metadata": {
    "id": "58075c85"
   },
   "outputs": [],
   "source": [
    "dfb.to_csv(\"TF-IDF_Baru.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27e00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "19-016_Dini Lestari Arianingsasi_Text Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
